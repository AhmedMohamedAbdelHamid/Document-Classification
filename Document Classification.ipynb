{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a423ebc0-a544-4022-a2b8-49f3d0febed5",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "## Glossary\n",
    "1- Import Libraries\n",
    "\n",
    "2- Utility Function\n",
    "\n",
    "3- helper Function\n",
    "\n",
    "4- FCNN Model\n",
    "\n",
    "5- Distilbert Model\n",
    "\n",
    "6- RoBERTa Model\n",
    "\n",
    "7- Extracting Features\n",
    "\n",
    "8- FCNN Model Training\n",
    "\n",
    "9- Distilbert Model Training\n",
    "\n",
    "10- RoBERTa Model Training\n",
    "\n",
    "11- Display the graph\n",
    "\n",
    "12- Save Dataset in MongoDB\n",
    "\n",
    "13- Save Models in MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5183be7-f3cd-4448-b806-79524b946a88",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf52f06d-94a0-46cf-885e-88c9e0abbd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Virus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Virus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Virus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb400df-26eb-43bf-89f9-431b547df0a1",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "309dddba-5aff-48ac-a14a-d95064223ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function\n",
    "def setup_gpu():\n",
    "    try:\n",
    "        # Prevent TensorFlow from taking all GPU memory\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"GPU setup completed\")\n",
    "        else:\n",
    "            print(\"No GPU devices found\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU setup failed: {e}\")\n",
    "        print(\"Falling back to CPU\")\n",
    "        # Disable GPU\n",
    "        tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "\n",
    "# Text preprocessing functions\n",
    "def setup_nltk():\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return stop_words, lemmatizer\n",
    "\n",
    "# Data preparation\n",
    "def prepare_data():\n",
    "    # Load dataset\n",
    "    path = kagglehub.dataset_download(\"alfathterry/bbc-full-text-document-classification\")\n",
    "    df = pd.read_csv(f\"{path}/bbc_data.csv\")\n",
    "    \n",
    "    # Clean data\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['processed_data'] = df_cleaned['data'].apply(preprocess_text)\n",
    "    \n",
    "    # Extract features\n",
    "    text_features = [text_analysis_helper(text) for text in tqdm(df_cleaned['data'], desc=\"Extracting features\")]\n",
    "    for key in text_features[0].keys():\n",
    "        df_cleaned[key] = [f[key] for f in text_features]\n",
    "    \n",
    "    # Add ratio features\n",
    "    df_cleaned['unique_word_ratio'] = df_cleaned['unique_words'] / df_cleaned['processed_word_count']\n",
    "    df_cleaned['preprocessing_reduction_ratio'] = df_cleaned['processed_word_count'] / df_cleaned['word_count']\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_cleaned['labels_encoded'] = label_encoder.fit_transform(df_cleaned['labels'])\n",
    "    \n",
    "    return df_cleaned, label_encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1c8b7-db0c-4d6e-830f-b28d0a00c126",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "618bee7b-1ba6-42c3-ae31-7287e70bc467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def preprocess_text(text):\n",
    "    stop_words, lemmatizer = setup_nltk()\n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(token) for token in tokens \n",
    "            if token.isalpha() and token not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def text_analysis_helper(text):\n",
    "    processed_text = preprocess_text(text)\n",
    "    words = processed_text.split()\n",
    "    raw_words = str(text).split()\n",
    "    \n",
    "    return {\n",
    "        'length': len(text),\n",
    "        'processed_length': len(processed_text),\n",
    "        'word_count': len(raw_words),\n",
    "        'processed_word_count': len(words),\n",
    "        'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
    "        'unique_words': len(set(words)),\n",
    "        'capital_letters': sum(1 for c in text if c.isupper()),\n",
    "        'punctuation_count': sum(1 for c in text if c in '.,!?'),\n",
    "        'stopwords_removed': len(raw_words) - len(words),\n",
    "        'lexical_density': len(set(words)) / len(words) if words else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b9bb5-a4d8-42f1-8621-922e372fcb3e",
   "metadata": {},
   "source": [
    "## FCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16773447-6bd2-4d94-975f-e3bc5e372619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCNN\n",
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        self.model.add(Dense(input_dim, input_shape=(input_dim,)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        # First hidden layer\n",
    "        self.model.add(Dense(512, activation='relu', \n",
    "                             kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)))\n",
    "        self.model.add(Dropout(0.4))\n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        # Second hidden layer\n",
    "        self.model.add(Dense(256, activation='relu',\n",
    "                             kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        # Third hidden layer\n",
    "        self.model.add(Dense(128, activation='relu',\n",
    "                             kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(BatchNormalization())\n",
    "        \n",
    "        # Output layer\n",
    "        self.model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # Fix: Use `self.model.summary()` to correctly call the summary method\n",
    "        self.model.summary()\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "    def train(self, X_train, y_train, epochs=100, batch_size=32):\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=3\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        return self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852efa59-2b7a-4cc2-a0dd-f77b43aa52e3",
   "metadata": {},
   "source": [
    "## Distilbert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfe52de-83a4-4c64-bb29-022af04c50bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distilbert Model\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class TransformerClassifier:\n",
    "    def __init__(self, num_labels):\n",
    "        self.model_name = \"distilbert-base-uncased\"  # First pre-trained model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def create_data_loaders(self, X_train, y_train, X_test, y_test, batch_size=16):\n",
    "        train_dataset = TextClassificationDataset(X_train, y_train, self.tokenizer)\n",
    "        valid_dataset = TextClassificationDataset(X_test, y_test, self.tokenizer)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader, valid_loader\n",
    "    \n",
    "    def train_epoch(self, data_loader, optimizer):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(data_loader, desc='Training'):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        actual_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc='Evaluating'):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                total_loss += outputs.loss.item()\n",
    "                predictions.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "                actual_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = np.mean(np.array(predictions) == np.array(actual_labels))\n",
    "        return total_loss / len(data_loader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db557f0-a219-42e7-9e57-3733cdba181a",
   "metadata": {},
   "source": [
    "## RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fbb96e6-ffac-4bdd-9671-a7adeee9aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa Model\n",
    "class RoBertaClassifier:\n",
    "    def __init__(self, num_labels):\n",
    "        self.model_name = \"roberta-base\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def create_data_loaders(self, X_train, y_train, X_test, y_test, batch_size=16):\n",
    "        train_dataset = TextClassificationDataset(X_train, y_train, self.tokenizer)\n",
    "        valid_dataset = TextClassificationDataset(X_test, y_test, self.tokenizer)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader, valid_loader\n",
    "    \n",
    "    def train_epoch(self, data_loader, optimizer):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(data_loader, desc='Training RoBERTa'):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return total_loss / len(data_loader)\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        actual_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc='Evaluating RoBERTa'):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                total_loss += outputs.loss.item()\n",
    "                predictions.extend(outputs.logits.argmax(dim=-1).cpu().numpy())\n",
    "                actual_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = np.mean(np.array(predictions) == np.array(actual_labels))\n",
    "        return total_loss / len(data_loader), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389d16f-e9bf-419e-8cf2-b5e7bc14bf75",
   "metadata": {},
   "source": [
    "## Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6be6659-6cb5-4083-99bb-20287e84e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU devices found\n",
      "Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 2225/2225 [00:10<00:00, 205.33it/s]\n"
     ]
    }
   ],
   "source": [
    "setup_gpu()\n",
    "print(\"Preparing data...\")\n",
    "df_cleaned, label_encoder = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f29fd-3397-4a09-8362-69051ca90fac",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "baed4191-4e96-41fb-b333-581d8994b5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>labels</th>\n",
       "      <th>processed_data</th>\n",
       "      <th>length</th>\n",
       "      <th>processed_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>processed_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>lexical_density</th>\n",
       "      <th>unique_word_ratio</th>\n",
       "      <th>preprocessing_reduction_ratio</th>\n",
       "      <th>labels_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Musicians to tackle US red tape  Musicians gro...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>musician tackle u red tape musician group tack...</td>\n",
       "      <td>2254</td>\n",
       "      <td>1439</td>\n",
       "      <td>378</td>\n",
       "      <td>205</td>\n",
       "      <td>6.024390</td>\n",
       "      <td>148</td>\n",
       "      <td>87</td>\n",
       "      <td>35</td>\n",
       "      <td>173</td>\n",
       "      <td>0.721951</td>\n",
       "      <td>0.721951</td>\n",
       "      <td>0.542328</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2s desire to be number one  U2, who have won ...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>desire number one three prestigious grammy awa...</td>\n",
       "      <td>4799</td>\n",
       "      <td>2902</td>\n",
       "      <td>838</td>\n",
       "      <td>427</td>\n",
       "      <td>5.798595</td>\n",
       "      <td>317</td>\n",
       "      <td>157</td>\n",
       "      <td>93</td>\n",
       "      <td>411</td>\n",
       "      <td>0.742389</td>\n",
       "      <td>0.742389</td>\n",
       "      <td>0.509547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rocker Doherty in on-stage fight  Rock singer ...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>rocker doherty fight rock singer pete doherty ...</td>\n",
       "      <td>2125</td>\n",
       "      <td>1383</td>\n",
       "      <td>358</td>\n",
       "      <td>195</td>\n",
       "      <td>6.097436</td>\n",
       "      <td>151</td>\n",
       "      <td>61</td>\n",
       "      <td>39</td>\n",
       "      <td>163</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.544693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snicket tops US box office chart  The film ada...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>snicket top u box office chart film adaptation...</td>\n",
       "      <td>1052</td>\n",
       "      <td>724</td>\n",
       "      <td>177</td>\n",
       "      <td>108</td>\n",
       "      <td>5.712963</td>\n",
       "      <td>84</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>69</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oceans Twelve raids box office  Oceans Twelve,...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "      <td>1598</td>\n",
       "      <td>1082</td>\n",
       "      <td>251</td>\n",
       "      <td>160</td>\n",
       "      <td>5.768750</td>\n",
       "      <td>128</td>\n",
       "      <td>78</td>\n",
       "      <td>41</td>\n",
       "      <td>91</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.637450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data         labels  \\\n",
       "0  Musicians to tackle US red tape  Musicians gro...  entertainment   \n",
       "1  U2s desire to be number one  U2, who have won ...  entertainment   \n",
       "2  Rocker Doherty in on-stage fight  Rock singer ...  entertainment   \n",
       "3  Snicket tops US box office chart  The film ada...  entertainment   \n",
       "4  Oceans Twelve raids box office  Oceans Twelve,...  entertainment   \n",
       "\n",
       "                                      processed_data  length  \\\n",
       "0  musician tackle u red tape musician group tack...    2254   \n",
       "1  desire number one three prestigious grammy awa...    4799   \n",
       "2  rocker doherty fight rock singer pete doherty ...    2125   \n",
       "3  snicket top u box office chart film adaptation...    1052   \n",
       "4  ocean twelve raid box office ocean twelve crim...    1598   \n",
       "\n",
       "   processed_length  word_count  processed_word_count  avg_word_length  \\\n",
       "0              1439         378                   205         6.024390   \n",
       "1              2902         838                   427         5.798595   \n",
       "2              1383         358                   195         6.097436   \n",
       "3               724         177                   108         5.712963   \n",
       "4              1082         251                   160         5.768750   \n",
       "\n",
       "   unique_words  capital_letters  punctuation_count  stopwords_removed  \\\n",
       "0           148               87                 35                173   \n",
       "1           317              157                 93                411   \n",
       "2           151               61                 39                163   \n",
       "3            84               49                 23                 69   \n",
       "4           128               78                 41                 91   \n",
       "\n",
       "   lexical_density  unique_word_ratio  preprocessing_reduction_ratio  \\\n",
       "0         0.721951           0.721951                       0.542328   \n",
       "1         0.742389           0.742389                       0.509547   \n",
       "2         0.774359           0.774359                       0.544693   \n",
       "3         0.777778           0.777778                       0.610169   \n",
       "4         0.800000           0.800000                       0.637450   \n",
       "\n",
       "   labels_encoded  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ecbdf132-6584-4980-9a59-a256130b4cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 16)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0fc32df5-d81d-469d-b2f2-81ac452ee437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>processed_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>processed_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>lexical_density</th>\n",
       "      <th>unique_word_ratio</th>\n",
       "      <th>preprocessing_reduction_ratio</th>\n",
       "      <th>labels_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2262.262472</td>\n",
       "      <td>1480.731685</td>\n",
       "      <td>384.166292</td>\n",
       "      <td>211.814831</td>\n",
       "      <td>5.991931</td>\n",
       "      <td>145.226517</td>\n",
       "      <td>74.214382</td>\n",
       "      <td>36.395955</td>\n",
       "      <td>172.351461</td>\n",
       "      <td>0.706947</td>\n",
       "      <td>0.706947</td>\n",
       "      <td>0.556379</td>\n",
       "      <td>1.958202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1361.230919</td>\n",
       "      <td>858.380987</td>\n",
       "      <td>238.141890</td>\n",
       "      <td>123.260260</td>\n",
       "      <td>0.321922</td>\n",
       "      <td>69.031291</td>\n",
       "      <td>44.908434</td>\n",
       "      <td>24.278660</td>\n",
       "      <td>116.962699</td>\n",
       "      <td>0.069232</td>\n",
       "      <td>0.069232</td>\n",
       "      <td>0.040482</td>\n",
       "      <td>1.428310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>502.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>4.776978</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.323671</td>\n",
       "      <td>0.323671</td>\n",
       "      <td>0.418803</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1448.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>5.766423</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.658892</td>\n",
       "      <td>0.658892</td>\n",
       "      <td>0.530815</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1965.000000</td>\n",
       "      <td>1298.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>5.989130</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>0.704348</td>\n",
       "      <td>0.704348</td>\n",
       "      <td>0.556391</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2801.000000</td>\n",
       "      <td>1841.000000</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>6.216102</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>0.754545</td>\n",
       "      <td>0.754545</td>\n",
       "      <td>0.581015</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25416.000000</td>\n",
       "      <td>15743.000000</td>\n",
       "      <td>4432.000000</td>\n",
       "      <td>2184.000000</td>\n",
       "      <td>7.006250</td>\n",
       "      <td>1076.000000</td>\n",
       "      <td>639.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>2248.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.804444</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             length  processed_length   word_count  processed_word_count  \\\n",
       "count   2225.000000       2225.000000  2225.000000           2225.000000   \n",
       "mean    2262.262472       1480.731685   384.166292            211.814831   \n",
       "std     1361.230919        858.380987   238.141890            123.260260   \n",
       "min      502.000000        333.000000    89.000000             47.000000   \n",
       "25%     1448.000000        950.000000   246.000000            137.000000   \n",
       "50%     1965.000000       1298.000000   332.000000            185.000000   \n",
       "75%     2801.000000       1841.000000   472.000000            262.000000   \n",
       "max    25416.000000      15743.000000  4432.000000           2184.000000   \n",
       "\n",
       "       avg_word_length  unique_words  capital_letters  punctuation_count  \\\n",
       "count      2225.000000   2225.000000      2225.000000        2225.000000   \n",
       "mean          5.991931    145.226517        74.214382          36.395955   \n",
       "std           0.321922     69.031291        44.908434          24.278660   \n",
       "min           4.776978     40.000000        14.000000           5.000000   \n",
       "25%           5.766423    100.000000        47.000000          23.000000   \n",
       "50%           5.989130    131.000000        65.000000          32.000000   \n",
       "75%           6.216102    175.000000        90.000000          43.000000   \n",
       "max           7.006250   1076.000000       639.000000         428.000000   \n",
       "\n",
       "       stopwords_removed  lexical_density  unique_word_ratio  \\\n",
       "count        2225.000000      2225.000000        2225.000000   \n",
       "mean          172.351461         0.706947           0.706947   \n",
       "std           116.962699         0.069232           0.069232   \n",
       "min            27.000000         0.323671           0.323671   \n",
       "25%           107.000000         0.658892           0.658892   \n",
       "50%           148.000000         0.704348           0.704348   \n",
       "75%           210.000000         0.754545           0.754545   \n",
       "max          2248.000000         0.947368           0.947368   \n",
       "\n",
       "       preprocessing_reduction_ratio  labels_encoded  \n",
       "count                    2225.000000     2225.000000  \n",
       "mean                        0.556379        1.958202  \n",
       "std                         0.040482        1.428310  \n",
       "min                         0.418803        0.000000  \n",
       "25%                         0.530815        1.000000  \n",
       "50%                         0.556391        2.000000  \n",
       "75%                         0.581015        3.000000  \n",
       "max                         0.804444        4.000000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c516c2b4-aff5-465a-bf20-9f25b40bf0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 16 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   data                           2225 non-null   object \n",
      " 1   labels                         2225 non-null   object \n",
      " 2   processed_data                 2225 non-null   object \n",
      " 3   length                         2225 non-null   int64  \n",
      " 4   processed_length               2225 non-null   int64  \n",
      " 5   word_count                     2225 non-null   int64  \n",
      " 6   processed_word_count           2225 non-null   int64  \n",
      " 7   avg_word_length                2225 non-null   float64\n",
      " 8   unique_words                   2225 non-null   int64  \n",
      " 9   capital_letters                2225 non-null   int64  \n",
      " 10  punctuation_count              2225 non-null   int64  \n",
      " 11  stopwords_removed              2225 non-null   int64  \n",
      " 12  lexical_density                2225 non-null   float64\n",
      " 13  unique_word_ratio              2225 non-null   float64\n",
      " 14  preprocessing_reduction_ratio  2225 non-null   float64\n",
      " 15  labels_encoded                 2225 non-null   int32  \n",
      "dtypes: float64(4), int32(1), int64(8), object(3)\n",
      "memory usage: 269.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81c94c01-c91c-468e-98a8-b970a374b23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>processed_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>processed_word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>capital_letters</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>lexical_density</th>\n",
       "      <th>unique_word_ratio</th>\n",
       "      <th>preprocessing_reduction_ratio</th>\n",
       "      <th>labels_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995160</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.994629</td>\n",
       "      <td>0.058389</td>\n",
       "      <td>0.970447</td>\n",
       "      <td>0.793505</td>\n",
       "      <td>0.937801</td>\n",
       "      <td>0.981760</td>\n",
       "      <td>-0.507161</td>\n",
       "      <td>-0.507161</td>\n",
       "      <td>-0.163908</td>\n",
       "      <td>0.183174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processed_length</th>\n",
       "      <td>0.995160</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987366</td>\n",
       "      <td>0.996387</td>\n",
       "      <td>0.092930</td>\n",
       "      <td>0.973657</td>\n",
       "      <td>0.805294</td>\n",
       "      <td>0.930353</td>\n",
       "      <td>0.960292</td>\n",
       "      <td>-0.524924</td>\n",
       "      <td>-0.524924</td>\n",
       "      <td>-0.089538</td>\n",
       "      <td>0.167985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.987366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991781</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.964368</td>\n",
       "      <td>0.791830</td>\n",
       "      <td>0.936808</td>\n",
       "      <td>0.990868</td>\n",
       "      <td>-0.496480</td>\n",
       "      <td>-0.496480</td>\n",
       "      <td>-0.200015</td>\n",
       "      <td>0.190388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processed_word_count</th>\n",
       "      <td>0.994629</td>\n",
       "      <td>0.996387</td>\n",
       "      <td>0.991781</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018583</td>\n",
       "      <td>0.972889</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>0.937707</td>\n",
       "      <td>0.965474</td>\n",
       "      <td>-0.529362</td>\n",
       "      <td>-0.529362</td>\n",
       "      <td>-0.095179</td>\n",
       "      <td>0.174149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_word_length</th>\n",
       "      <td>0.058389</td>\n",
       "      <td>0.092930</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.018583</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041010</td>\n",
       "      <td>-0.117675</td>\n",
       "      <td>-0.014635</td>\n",
       "      <td>-0.006361</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>0.088533</td>\n",
       "      <td>-0.098160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_words</th>\n",
       "      <td>0.970447</td>\n",
       "      <td>0.973657</td>\n",
       "      <td>0.964368</td>\n",
       "      <td>0.972889</td>\n",
       "      <td>0.041010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796916</td>\n",
       "      <td>0.912652</td>\n",
       "      <td>0.938229</td>\n",
       "      <td>-0.414305</td>\n",
       "      <td>-0.414305</td>\n",
       "      <td>-0.117179</td>\n",
       "      <td>0.181620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital_letters</th>\n",
       "      <td>0.793505</td>\n",
       "      <td>0.805294</td>\n",
       "      <td>0.791830</td>\n",
       "      <td>0.821662</td>\n",
       "      <td>-0.117675</td>\n",
       "      <td>0.796916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811251</td>\n",
       "      <td>0.746304</td>\n",
       "      <td>-0.435587</td>\n",
       "      <td>-0.435587</td>\n",
       "      <td>0.086093</td>\n",
       "      <td>0.114178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation_count</th>\n",
       "      <td>0.937801</td>\n",
       "      <td>0.930353</td>\n",
       "      <td>0.936808</td>\n",
       "      <td>0.937707</td>\n",
       "      <td>-0.014635</td>\n",
       "      <td>0.912652</td>\n",
       "      <td>0.811251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>-0.441101</td>\n",
       "      <td>-0.441101</td>\n",
       "      <td>-0.109504</td>\n",
       "      <td>0.125616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopwords_removed</th>\n",
       "      <td>0.981760</td>\n",
       "      <td>0.960292</td>\n",
       "      <td>0.990868</td>\n",
       "      <td>0.965474</td>\n",
       "      <td>-0.006361</td>\n",
       "      <td>0.938229</td>\n",
       "      <td>0.746304</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.452994</td>\n",
       "      <td>-0.452994</td>\n",
       "      <td>-0.306937</td>\n",
       "      <td>0.204115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_density</th>\n",
       "      <td>-0.507161</td>\n",
       "      <td>-0.524924</td>\n",
       "      <td>-0.496480</td>\n",
       "      <td>-0.529362</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>-0.414305</td>\n",
       "      <td>-0.435587</td>\n",
       "      <td>-0.441101</td>\n",
       "      <td>-0.452994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.077857</td>\n",
       "      <td>-0.100969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_word_ratio</th>\n",
       "      <td>-0.507161</td>\n",
       "      <td>-0.524924</td>\n",
       "      <td>-0.496480</td>\n",
       "      <td>-0.529362</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>-0.414305</td>\n",
       "      <td>-0.435587</td>\n",
       "      <td>-0.441101</td>\n",
       "      <td>-0.452994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.077857</td>\n",
       "      <td>-0.100969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preprocessing_reduction_ratio</th>\n",
       "      <td>-0.163908</td>\n",
       "      <td>-0.089538</td>\n",
       "      <td>-0.200015</td>\n",
       "      <td>-0.095179</td>\n",
       "      <td>0.088533</td>\n",
       "      <td>-0.117179</td>\n",
       "      <td>0.086093</td>\n",
       "      <td>-0.109504</td>\n",
       "      <td>-0.306937</td>\n",
       "      <td>-0.077857</td>\n",
       "      <td>-0.077857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.223811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels_encoded</th>\n",
       "      <td>0.183174</td>\n",
       "      <td>0.167985</td>\n",
       "      <td>0.190388</td>\n",
       "      <td>0.174149</td>\n",
       "      <td>-0.098160</td>\n",
       "      <td>0.181620</td>\n",
       "      <td>0.114178</td>\n",
       "      <td>0.125616</td>\n",
       "      <td>0.204115</td>\n",
       "      <td>-0.100969</td>\n",
       "      <td>-0.100969</td>\n",
       "      <td>-0.223811</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 length  processed_length  word_count  \\\n",
       "length                         1.000000          0.995160    0.997000   \n",
       "processed_length               0.995160          1.000000    0.987366   \n",
       "word_count                     0.997000          0.987366    1.000000   \n",
       "processed_word_count           0.994629          0.996387    0.991781   \n",
       "avg_word_length                0.058389          0.092930    0.006494   \n",
       "unique_words                   0.970447          0.973657    0.964368   \n",
       "capital_letters                0.793505          0.805294    0.791830   \n",
       "punctuation_count              0.937801          0.930353    0.936808   \n",
       "stopwords_removed              0.981760          0.960292    0.990868   \n",
       "lexical_density               -0.507161         -0.524924   -0.496480   \n",
       "unique_word_ratio             -0.507161         -0.524924   -0.496480   \n",
       "preprocessing_reduction_ratio -0.163908         -0.089538   -0.200015   \n",
       "labels_encoded                 0.183174          0.167985    0.190388   \n",
       "\n",
       "                               processed_word_count  avg_word_length  \\\n",
       "length                                     0.994629         0.058389   \n",
       "processed_length                           0.996387         0.092930   \n",
       "word_count                                 0.991781         0.006494   \n",
       "processed_word_count                       1.000000         0.018583   \n",
       "avg_word_length                            0.018583         1.000000   \n",
       "unique_words                               0.972889         0.041010   \n",
       "capital_letters                            0.821662        -0.117675   \n",
       "punctuation_count                          0.937707        -0.014635   \n",
       "stopwords_removed                          0.965474        -0.006361   \n",
       "lexical_density                           -0.529362         0.016759   \n",
       "unique_word_ratio                         -0.529362         0.016759   \n",
       "preprocessing_reduction_ratio             -0.095179         0.088533   \n",
       "labels_encoded                             0.174149        -0.098160   \n",
       "\n",
       "                               unique_words  capital_letters  \\\n",
       "length                             0.970447         0.793505   \n",
       "processed_length                   0.973657         0.805294   \n",
       "word_count                         0.964368         0.791830   \n",
       "processed_word_count               0.972889         0.821662   \n",
       "avg_word_length                    0.041010        -0.117675   \n",
       "unique_words                       1.000000         0.796916   \n",
       "capital_letters                    0.796916         1.000000   \n",
       "punctuation_count                  0.912652         0.811251   \n",
       "stopwords_removed                  0.938229         0.746304   \n",
       "lexical_density                   -0.414305        -0.435587   \n",
       "unique_word_ratio                 -0.414305        -0.435587   \n",
       "preprocessing_reduction_ratio     -0.117179         0.086093   \n",
       "labels_encoded                     0.181620         0.114178   \n",
       "\n",
       "                               punctuation_count  stopwords_removed  \\\n",
       "length                                  0.937801           0.981760   \n",
       "processed_length                        0.930353           0.960292   \n",
       "word_count                              0.936808           0.990868   \n",
       "processed_word_count                    0.937707           0.965474   \n",
       "avg_word_length                        -0.014635          -0.006361   \n",
       "unique_words                            0.912652           0.938229   \n",
       "capital_letters                         0.811251           0.746304   \n",
       "punctuation_count                       1.000000           0.919192   \n",
       "stopwords_removed                       0.919192           1.000000   \n",
       "lexical_density                        -0.441101          -0.452994   \n",
       "unique_word_ratio                      -0.441101          -0.452994   \n",
       "preprocessing_reduction_ratio          -0.109504          -0.306937   \n",
       "labels_encoded                          0.125616           0.204115   \n",
       "\n",
       "                               lexical_density  unique_word_ratio  \\\n",
       "length                               -0.507161          -0.507161   \n",
       "processed_length                     -0.524924          -0.524924   \n",
       "word_count                           -0.496480          -0.496480   \n",
       "processed_word_count                 -0.529362          -0.529362   \n",
       "avg_word_length                       0.016759           0.016759   \n",
       "unique_words                         -0.414305          -0.414305   \n",
       "capital_letters                      -0.435587          -0.435587   \n",
       "punctuation_count                    -0.441101          -0.441101   \n",
       "stopwords_removed                    -0.452994          -0.452994   \n",
       "lexical_density                       1.000000           1.000000   \n",
       "unique_word_ratio                     1.000000           1.000000   \n",
       "preprocessing_reduction_ratio        -0.077857          -0.077857   \n",
       "labels_encoded                       -0.100969          -0.100969   \n",
       "\n",
       "                               preprocessing_reduction_ratio  labels_encoded  \n",
       "length                                             -0.163908        0.183174  \n",
       "processed_length                                   -0.089538        0.167985  \n",
       "word_count                                         -0.200015        0.190388  \n",
       "processed_word_count                               -0.095179        0.174149  \n",
       "avg_word_length                                     0.088533       -0.098160  \n",
       "unique_words                                       -0.117179        0.181620  \n",
       "capital_letters                                     0.086093        0.114178  \n",
       "punctuation_count                                  -0.109504        0.125616  \n",
       "stopwords_removed                                  -0.306937        0.204115  \n",
       "lexical_density                                    -0.077857       -0.100969  \n",
       "unique_word_ratio                                  -0.077857       -0.100969  \n",
       "preprocessing_reduction_ratio                       1.000000       -0.223811  \n",
       "labels_encoded                                     -0.223811        1.000000  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_df = df_cleaned.select_dtypes(include=['number'])\n",
    "\n",
    "numeric_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19c5f9-4c73-4627-8911-008c10987b5e",
   "metadata": {},
   "source": [
    "## FCNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e84ff06-e03d-4c3a-87ae-95e7091b0381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU devices found\n",
      "Preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 2225/2225 [00:10<00:00, 208.67it/s]\n",
      "C:\\Users\\Virus\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Fully Connected Neural Network...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │           \u001b[38;5;34m156\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │            \u001b[38;5;34m48\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m6,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m645\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">175,313</span> (684.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m175,313\u001b[0m (684.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">173,497</span> (677.72 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m173,497\u001b[0m (677.72 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,816</span> (7.09 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,816\u001b[0m (7.09 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.2915 - loss: 2.8454 - val_accuracy: 0.3933 - val_loss: 2.3763 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3950 - loss: 2.4373 - val_accuracy: 0.4045 - val_loss: 2.3310 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3822 - loss: 2.4724 - val_accuracy: 0.4157 - val_loss: 2.2812 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4423 - loss: 2.3161 - val_accuracy: 0.4551 - val_loss: 2.2328 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4309 - loss: 2.3051 - val_accuracy: 0.4691 - val_loss: 2.1849 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4555 - loss: 2.2700 - val_accuracy: 0.4803 - val_loss: 2.1253 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4512 - loss: 2.2648 - val_accuracy: 0.5225 - val_loss: 2.1005 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4614 - loss: 2.1984 - val_accuracy: 0.5365 - val_loss: 2.0574 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4757 - loss: 2.1555 - val_accuracy: 0.5309 - val_loss: 2.0362 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4417 - loss: 2.2407 - val_accuracy: 0.5365 - val_loss: 2.0153 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4916 - loss: 2.0838 - val_accuracy: 0.5309 - val_loss: 2.0001 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4841 - loss: 2.1133 - val_accuracy: 0.5225 - val_loss: 1.9845 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5166 - loss: 2.1063 - val_accuracy: 0.5112 - val_loss: 1.9845 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4883 - loss: 2.1434 - val_accuracy: 0.4972 - val_loss: 1.9730 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4864 - loss: 2.1207 - val_accuracy: 0.5169 - val_loss: 1.9690 - learning_rate: 5.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5225 - loss: 2.0104 - val_accuracy: 0.5365 - val_loss: 1.9587 - learning_rate: 5.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5023 - loss: 2.0619 - val_accuracy: 0.5197 - val_loss: 1.9453 - learning_rate: 5.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5241 - loss: 2.0053 - val_accuracy: 0.5365 - val_loss: 1.9580 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5512 - loss: 1.9378 - val_accuracy: 0.5421 - val_loss: 1.9378 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5097 - loss: 2.0227 - val_accuracy: 0.5197 - val_loss: 1.9392 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5223 - loss: 1.9514 - val_accuracy: 0.5225 - val_loss: 1.9228 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5017 - loss: 1.9973 - val_accuracy: 0.5084 - val_loss: 1.9365 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4973 - loss: 2.0056 - val_accuracy: 0.5534 - val_loss: 1.9011 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5309 - loss: 1.9704 - val_accuracy: 0.5337 - val_loss: 1.8976 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5389 - loss: 1.9305 - val_accuracy: 0.5225 - val_loss: 1.8993 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5228 - loss: 1.9315 - val_accuracy: 0.5056 - val_loss: 1.8965 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5171 - loss: 1.9418 - val_accuracy: 0.5225 - val_loss: 1.8807 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5302 - loss: 1.9399 - val_accuracy: 0.5225 - val_loss: 1.8872 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5623 - loss: 1.9003 - val_accuracy: 0.5253 - val_loss: 1.8794 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5207 - loss: 1.9191 - val_accuracy: 0.5337 - val_loss: 1.8788 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5580 - loss: 1.8611 - val_accuracy: 0.5281 - val_loss: 1.8598 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5280 - loss: 1.9081 - val_accuracy: 0.5365 - val_loss: 1.8527 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5276 - loss: 1.8824 - val_accuracy: 0.5393 - val_loss: 1.8480 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5310 - loss: 1.8703 - val_accuracy: 0.5337 - val_loss: 1.8393 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5512 - loss: 1.8851 - val_accuracy: 0.5140 - val_loss: 1.8445 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5570 - loss: 1.8712 - val_accuracy: 0.5197 - val_loss: 1.8350 - learning_rate: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5358 - loss: 1.8383 - val_accuracy: 0.5281 - val_loss: 1.8197 - learning_rate: 5.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5342 - loss: 1.8275 - val_accuracy: 0.5309 - val_loss: 1.8178 - learning_rate: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5423 - loss: 1.8799 - val_accuracy: 0.5281 - val_loss: 1.8087 - learning_rate: 5.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5670 - loss: 1.8004 - val_accuracy: 0.5309 - val_loss: 1.8085 - learning_rate: 5.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5619 - loss: 1.8079 - val_accuracy: 0.5281 - val_loss: 1.8040 - learning_rate: 5.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5510 - loss: 1.7933 - val_accuracy: 0.5365 - val_loss: 1.7852 - learning_rate: 5.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5603 - loss: 1.7913 - val_accuracy: 0.5421 - val_loss: 1.7814 - learning_rate: 5.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5582 - loss: 1.7461 - val_accuracy: 0.5253 - val_loss: 1.7682 - learning_rate: 5.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5454 - loss: 1.7779 - val_accuracy: 0.5253 - val_loss: 1.7757 - learning_rate: 5.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5860 - loss: 1.7316 - val_accuracy: 0.5478 - val_loss: 1.7678 - learning_rate: 5.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5728 - loss: 1.7248 - val_accuracy: 0.5365 - val_loss: 1.7506 - learning_rate: 5.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5519 - loss: 1.7504 - val_accuracy: 0.5478 - val_loss: 1.7409 - learning_rate: 5.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5968 - loss: 1.6936 - val_accuracy: 0.5421 - val_loss: 1.7387 - learning_rate: 5.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5734 - loss: 1.7409 - val_accuracy: 0.5393 - val_loss: 1.7469 - learning_rate: 5.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5691 - loss: 1.7022 - val_accuracy: 0.5393 - val_loss: 1.7301 - learning_rate: 5.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5685 - loss: 1.7118 - val_accuracy: 0.5506 - val_loss: 1.7268 - learning_rate: 5.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5782 - loss: 1.6700 - val_accuracy: 0.5478 - val_loss: 1.7014 - learning_rate: 5.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5923 - loss: 1.6599 - val_accuracy: 0.5281 - val_loss: 1.7162 - learning_rate: 5.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5792 - loss: 1.6650 - val_accuracy: 0.5478 - val_loss: 1.6985 - learning_rate: 5.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5661 - loss: 1.6811 - val_accuracy: 0.5449 - val_loss: 1.6932 - learning_rate: 5.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5526 - loss: 1.6817 - val_accuracy: 0.5506 - val_loss: 1.6796 - learning_rate: 5.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5803 - loss: 1.6304 - val_accuracy: 0.5478 - val_loss: 1.6697 - learning_rate: 5.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5784 - loss: 1.6420 - val_accuracy: 0.5281 - val_loss: 1.6771 - learning_rate: 5.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5682 - loss: 1.6599 - val_accuracy: 0.5393 - val_loss: 1.6832 - learning_rate: 5.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5290 - loss: 1.7113 - val_accuracy: 0.5534 - val_loss: 1.6564 - learning_rate: 5.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5648 - loss: 1.6480 - val_accuracy: 0.5393 - val_loss: 1.6654 - learning_rate: 5.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5800 - loss: 1.6172 - val_accuracy: 0.5197 - val_loss: 1.6501 - learning_rate: 5.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5553 - loss: 1.6733 - val_accuracy: 0.5562 - val_loss: 1.6384 - learning_rate: 5.0000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5984 - loss: 1.5578 - val_accuracy: 0.5534 - val_loss: 1.6441 - learning_rate: 5.0000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5844 - loss: 1.5757 - val_accuracy: 0.5478 - val_loss: 1.6333 - learning_rate: 5.0000e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5957 - loss: 1.5886 - val_accuracy: 0.5169 - val_loss: 1.6383 - learning_rate: 5.0000e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5827 - loss: 1.5527 - val_accuracy: 0.5421 - val_loss: 1.6249 - learning_rate: 5.0000e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5700 - loss: 1.5501 - val_accuracy: 0.5309 - val_loss: 1.6257 - learning_rate: 5.0000e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6122 - loss: 1.5137 - val_accuracy: 0.5253 - val_loss: 1.6230 - learning_rate: 5.0000e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5487 - loss: 1.6032 - val_accuracy: 0.5225 - val_loss: 1.6098 - learning_rate: 5.0000e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6022 - loss: 1.5301 - val_accuracy: 0.5337 - val_loss: 1.5989 - learning_rate: 5.0000e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5972 - loss: 1.5471 - val_accuracy: 0.5365 - val_loss: 1.5964 - learning_rate: 5.0000e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5694 - loss: 1.5458 - val_accuracy: 0.5337 - val_loss: 1.5882 - learning_rate: 5.0000e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6039 - loss: 1.5403 - val_accuracy: 0.5253 - val_loss: 1.5893 - learning_rate: 5.0000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5701 - loss: 1.5958 - val_accuracy: 0.5365 - val_loss: 1.5807 - learning_rate: 5.0000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5678 - loss: 1.5731 - val_accuracy: 0.5309 - val_loss: 1.5860 - learning_rate: 5.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6028 - loss: 1.5006 - val_accuracy: 0.5337 - val_loss: 1.5794 - learning_rate: 5.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6090 - loss: 1.4893 - val_accuracy: 0.5197 - val_loss: 1.5912 - learning_rate: 5.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6075 - loss: 1.4634 - val_accuracy: 0.5281 - val_loss: 1.5759 - learning_rate: 5.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5935 - loss: 1.4980 - val_accuracy: 0.5225 - val_loss: 1.5678 - learning_rate: 5.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5674 - loss: 1.5210 - val_accuracy: 0.5421 - val_loss: 1.5554 - learning_rate: 5.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6140 - loss: 1.4644 - val_accuracy: 0.5534 - val_loss: 1.5593 - learning_rate: 5.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5958 - loss: 1.4820 - val_accuracy: 0.5140 - val_loss: 1.5562 - learning_rate: 5.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6019 - loss: 1.4863 - val_accuracy: 0.5197 - val_loss: 1.5560 - learning_rate: 5.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5693 - loss: 1.4848 - val_accuracy: 0.5281 - val_loss: 1.5473 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5572 - loss: 1.4999 - val_accuracy: 0.5309 - val_loss: 1.5421 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6100 - loss: 1.4211 - val_accuracy: 0.5337 - val_loss: 1.5383 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5965 - loss: 1.4524 - val_accuracy: 0.5393 - val_loss: 1.5343 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6275 - loss: 1.4046 - val_accuracy: 0.5421 - val_loss: 1.5342 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6193 - loss: 1.3999 - val_accuracy: 0.5449 - val_loss: 1.5308 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5929 - loss: 1.4689 - val_accuracy: 0.5393 - val_loss: 1.5271 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5923 - loss: 1.4319 - val_accuracy: 0.5393 - val_loss: 1.5258 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5803 - loss: 1.4497 - val_accuracy: 0.5393 - val_loss: 1.5310 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6141 - loss: 1.4250 - val_accuracy: 0.5365 - val_loss: 1.5240 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6089 - loss: 1.4228 - val_accuracy: 0.5421 - val_loss: 1.5245 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6036 - loss: 1.4384 - val_accuracy: 0.5421 - val_loss: 1.5207 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5832 - loss: 1.4453 - val_accuracy: 0.5421 - val_loss: 1.5200 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6113 - loss: 1.4117 - val_accuracy: 0.5421 - val_loss: 1.5179 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6219 - loss: 1.4413 - val_accuracy: 0.5478 - val_loss: 1.5149 - learning_rate: 1.0000e-04\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5880 - loss: 1.4973 \n",
      "\n",
      "FCNN Test Accuracy: 0.5663\n"
     ]
    }
   ],
   "source": [
    "# Feature columns for fully connected network\n",
    "feature_columns = [\n",
    "    'length', 'processed_length', 'word_count', 'processed_word_count',\n",
    "    'avg_word_length', 'unique_words', 'capital_letters', 'punctuation_count',\n",
    "    'stopwords_removed', 'lexical_density', 'unique_word_ratio',\n",
    "    'preprocessing_reduction_ratio'\n",
    "]\n",
    "    \n",
    "# Prepare features and labels for FCNN\n",
    "X = df_cleaned[feature_columns].values\n",
    "y = df_cleaned['labels_encoded'].values\n",
    "    \n",
    "# Split data for FCNN\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "    \n",
    "# Scale features for FCNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "print(\"\\nTraining Fully Connected Neural Network...\")\n",
    "fcnn_classifier = NeuralNetworkClassifier(\n",
    "    input_dim=len(feature_columns),\n",
    "    num_classes=len(label_encoder.classes_)\n",
    ")\n",
    "fcnn_history = fcnn_classifier.train(X_train_scaled, y_train)\n",
    "    \n",
    "# Evaluate FCNN\n",
    "fcnn_test_loss, fcnn_test_accuracy = fcnn_classifier.model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"\\nFCNN Test Accuracy: {fcnn_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423180d7-16d7-4bea-88e5-cac093345203",
   "metadata": {},
   "source": [
    "## DistilBERT Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea8f9103-2406-4f48-9bf4-ae5cb9812f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training DistilBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBERT Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 112/112 [32:01<00:00, 17.16s/it]\n",
      "Evaluating: 100%|██████████| 28/28 [02:10<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Train Loss: 0.6269\n",
      "DistilBERT Val Loss: 0.1393\n",
      "DistilBERT Val Accuracy: 0.9753\n",
      "\n",
      "DistilBERT Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 112/112 [34:54<00:00, 18.70s/it]\n",
      "Evaluating: 100%|██████████| 28/28 [02:38<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Train Loss: 0.0965\n",
      "DistilBERT Val Loss: 0.0769\n",
      "DistilBERT Val Accuracy: 0.9843\n",
      "\n",
      "DistilBERT Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 112/112 [34:17<00:00, 18.37s/it]\n",
      "Evaluating: 100%|██████████| 28/28 [02:21<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Train Loss: 0.0434\n",
      "DistilBERT Val Loss: 0.0842\n",
      "DistilBERT Val Accuracy: 0.9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformer setup\n",
    "X_text = df_cleaned['data'].values\n",
    "y = df_cleaned['labels_encoded'].values\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "    \n",
    "print(\"\\nTraining DistilBERT...\")\n",
    "distilbert_classifier = TransformerClassifier(num_labels=len(label_encoder.classes_))\n",
    "distil_train_loader, distil_valid_loader = distilbert_classifier.create_data_loaders(\n",
    "    X_train_text, y_train, X_test_text, y_test\n",
    ")\n",
    "    \n",
    "distil_optimizer = torch.optim.AdamW(distilbert_classifier.model.parameters(), lr=2e-5)\n",
    "best_accuracy_distil = 0\n",
    "    \n",
    "for epoch in range(3):\n",
    "    print(f\"\\nDistilBERT Epoch {epoch + 1}/3\")\n",
    "    train_loss = distilbert_classifier.train_epoch(distil_train_loader, distil_optimizer)\n",
    "    val_loss, val_accuracy = distilbert_classifier.evaluate(distil_valid_loader)\n",
    "    \n",
    "    print(f\"DistilBERT Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"DistilBERT Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"DistilBERT Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    if val_accuracy > best_accuracy_distil:\n",
    "        best_accuracy_distil = val_accuracy\n",
    "        torch.save(distilbert_classifier.model.state_dict(), 'best_distilbert_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1296d84-98d4-43f8-94bc-2ba5e4125dba",
   "metadata": {},
   "source": [
    "## RoBERTa Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdd5351d-4ca5-4913-9210-fab347b0f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RoBERTa Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training RoBERTa: 100%|██████████| 112/112 [1:25:53<00:00, 46.01s/it]\n",
      "Evaluating RoBERTa: 100%|██████████| 28/28 [05:32<00:00, 11.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Train Loss: 0.4739\n",
      "RoBERTa Val Loss: 0.1070\n",
      "RoBERTa Val Accuracy: 0.9708\n",
      "\n",
      "RoBERTa Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training RoBERTa: 100%|██████████| 112/112 [1:25:06<00:00, 45.60s/it]\n",
      "Evaluating RoBERTa: 100%|██████████| 28/28 [04:35<00:00,  9.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Train Loss: 0.0618\n",
      "RoBERTa Val Loss: 0.1858\n",
      "RoBERTa Val Accuracy: 0.9371\n",
      "\n",
      "RoBERTa Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training RoBERTa: 100%|██████████| 112/112 [1:23:23<00:00, 44.67s/it]\n",
      "Evaluating RoBERTa: 100%|██████████| 28/28 [04:15<00:00,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Train Loss: 0.0435\n",
      "RoBERTa Val Loss: 0.0804\n",
      "RoBERTa Val Accuracy: 0.9820\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining RoBERTa...\")\n",
    "roberta_classifier = RoBertaClassifier(num_labels=len(label_encoder.classes_))\n",
    "roberta_train_loader, roberta_valid_loader = roberta_classifier.create_data_loaders(\n",
    "    X_train_text, y_train, X_test_text, y_test\n",
    ")\n",
    "    \n",
    "roberta_optimizer = torch.optim.AdamW(roberta_classifier.model.parameters(), lr=2e-5)\n",
    "best_accuracy_roberta = 0\n",
    "    \n",
    "for epoch in range(3):\n",
    "    print(f\"\\nRoBERTa Epoch {epoch + 1}/3\")\n",
    "    train_loss = roberta_classifier.train_epoch(roberta_train_loader, roberta_optimizer)\n",
    "    val_loss, val_accuracy = roberta_classifier.evaluate(roberta_valid_loader)\n",
    "        \n",
    "    print(f\"RoBERTa Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"RoBERTa Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"RoBERTa Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    if val_accuracy > best_accuracy_roberta:\n",
    "        best_accuracy_roberta = val_accuracy\n",
    "        torch.save(roberta_classifier.model.state_dict(), 'best_roberta_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c32108ed-6f7e-4a9c-9c8a-37cd88dd1882",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_accuracy_distil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest DistilBERT Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_accuracy_distil\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest RoBERTa Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_accuracy_roberta\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest FCNN Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfcnn_test_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_accuracy_distil' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBest DistilBERT Accuracy: {best_accuracy_distil:.4f}\")\n",
    "print(f\"Best RoBERTa Accuracy: {best_accuracy_roberta:.4f}\")\n",
    "print(f\"Best FCNN Accuracy: {fcnn_test_accuracy:.4f}\")\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8505de-1929-47f9-b021-2daa82bdb04b",
   "metadata": {},
   "source": [
    "## Display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "308bd79e-0e47-4ab4-9eff-23e1b6f6fc1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_accuracy_distil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistilBERT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoBERTa\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFCNN\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m [best_accuracy_distil, best_accuracy_roberta, fcnn_test_accuracy]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plotting the graph\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_accuracy_distil' is not defined"
     ]
    }
   ],
   "source": [
    "models = ['DistilBERT', 'RoBERTa', 'FCNN']\n",
    "accuracies = [best_accuracy_distil, best_accuracy_roberta, fcnn_test_accuracy]\n",
    "\n",
    "# Plotting the graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(models, accuracies, color=['skyblue', 'orange', 'green'], edgecolor='black')\n",
    "\n",
    "# Adding details to the graph\n",
    "plt.title('Model Accuracy Comparison', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.ylim(0, 1)  # Accuracy range is typically 0 to 1\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.02, f\"{acc:.2f}\", ha='center', fontsize=12, color='black')\n",
    "\n",
    "# Display the graph\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4f13b-1b2f-4798-a8a2-fb71ce765fdf",
   "metadata": {},
   "source": [
    "## Save Dataset in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1cf25-6068-4138-a423-752a9330eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the database (create it if it doesn't exist)\n",
    "db = client[\"DLDB\"]\n",
    "\n",
    "# Access the collection (create it if it doesn't exist)\n",
    "collection = db[\"Dataset\"]\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries\n",
    "data = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Insert the data into the collection\n",
    "collection.insert_many(data)\n",
    "\n",
    "print(\"Dataset successfully saved to MongoDB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26df829e-4cc8-4df3-b598-842c1dda9585",
   "metadata": {},
   "source": [
    "## Save Models in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8a6c004a-6904-4299-83de-e49bc98df78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully in MongoDB!\n"
     ]
    }
   ],
   "source": [
    "model_binary = pickle.dumps(fcnn_classifier)\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")  # Connect to local MongoDB\n",
    "db = client['ml_models']  # Database\n",
    "collection = db['models']  # Collection\n",
    "\n",
    "model_document = {\n",
    "    \"model_name\": \"FCNN Model\",\n",
    "    \"model_binary\": model_binary\n",
    "}\n",
    "\n",
    "model_document = {\n",
    "    \"model_name\": \"DistilBERT Model\",\n",
    "    \"framework\": \"scikit-learn\",\n",
    "    \"model_binary\": model_binary\n",
    "}\n",
    "\n",
    "model_document = {\n",
    "    \"model_name\": \"RoBERTa Model\",\n",
    "    \"framework\": \"scikit-learn\",\n",
    "    \"model_binary\": model_binary\n",
    "}\n",
    "\n",
    "collection.insert_one(model_document)\n",
    "\n",
    "print(\"Model saved successfully in MongoDB!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
